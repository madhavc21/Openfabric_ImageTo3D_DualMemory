# AI Creative Partner: End-to-End Generative Pipeline

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This project implements an intelligent, end-to-end pipeline powered by local Large Language Models (LLMs) and Openfabric services to transform simple text ideas into 2D images and then into 3D models, complete with robust short-term and long-term memory capabilities. It aims to act as a "creative partner," understanding user requests contextually and recalling past creations.

**Core Features:**
*   [LLM-Powered Understanding](#llm-powered-understanding)
*   [Image and 3D Model Generation](#image-and-3d-model-generation)
*   [Advanced Memory System](#advanced-memory-system)
    *   [Short-Term Contextual Memory](#short-term-contextual-memory)
    *   [Long-Term Persistent Memory](#long-term-persistent-memory)
    *   [Semantic Recall with FAISS](#semantic-recall-with-faiss)
*   [Dynamic Openfabric Service Interaction](#dynamic-openfabric-service-interaction)
*   [Dockerized Deployment](#dockerized-deployment)
*   [Gradio UI](#gradio-ui)

---

## Table of Contents

*   [Overview](#overview)
*   [Features Detailed](#features-detailed)
    *   [LLM-Powered Understanding](#llm-powered-understanding)
    *   [Image and 3D Model Generation](#image-and-3d-model-generation)
    *   [Advanced Memory System](#advanced-memory-system)
*   [Technology Stack](#technology-stack)
*   [Project Structure](#project-structure)
*   [Setup and Installation](#setup-and-installation)
    *   [Prerequisites](#prerequisites)
    *   [Local Setup (using Poetry)](#local-setup-using-poetry)
    *   [Ollama LLM Setup](#ollama-llm-setup)
*   [Running the Application](#running-the-application)
    *   [1. As an Openfabric Application (Core)](#1-as-an-openfabric-application-core)
        *   [Using Docker (Recommended)](#using-docker-recommended)
        *   [Running Locally with `start.sh`](#running-locally-with-startsh)
        *   [Testing via Swagger UI](#testing-via-swagger-ui)
    *   [2. Using the Gradio UI](#2-using-the-gradio-ui)
*   [Configuration](#configuration)
*   [Troubleshooting](#troubleshooting)
*   [Future Enhancements](#future-enhancements)
*   [Contributing](#contributing)
*   [License](#license)

---

## Overview

This application serves as a demonstration of building a complex AI pipeline. A user provides a text prompt, which is first interpreted and enhanced by a local LLM. The enhanced prompt then drives an Openfabric Text-to-Image service, and the resulting image is subsequently fed into an Openfabric Image-to-3D service. The system remembers all creations, allowing users to recall and request modifications to past work using natural language.

The primary interface for this application (as per core requirements) is an API endpoint exposed when run as an Openfabric application. A Gradio-based chat UI is also provided as a user-friendly way to interact with the pipeline and showcase its conversational memory features.

<video controls width="640">
  <source src="Gradio-demo.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>


## Features Detailed

### LLM-Powered Understanding
The pipeline begins with a local LLM (e.g., DeepSeek, Llama, Gemma via Ollama) to process user input:
*   **Intent Parsing:** Determines if the user wants a new creation, a modification of a recent item (short-term context), or to recall and modify a past creation (long-term memory).
*   **Entity Extraction:** For recall requests, it extracts keywords and date references (e.g., "my dragon from last Tuesday").
*   **Creative Prompt Enhancement:** For new creations, it expands the user's idea into a rich, descriptive prompt suitable for image generation.
*   **Contextual Prompt Generation:** For modifications, it combines the context of a previous creation with the new modification instruction to synthesize a new prompt.
*   The LLM interaction is designed to produce structured JSON output for reliable integration into the Python pipeline.

### Image and 3D Model Generation
The system chains two distinct Openfabric services:
1.  **Text-to-Image Service:**
    *   Receives the LLM-generated/enhanced prompt.
    *   Generates a 2D image.
    *   App ID (Hostname): `c25dcd829d134ea98f5ae4dd311d13bc.node3.openfabric.network` (Example, confirm from config)
2.  **Image-to-3D Service:**
    *   Receives the image generated by the first service.
    *   Transforms it into a 3D model (GLB format) and an optional preview video.
    *   App ID (Hostname): `f0b5f319156c4819b9827000b17e511a.node3.openfabric.network` (Example, confirm from config)

Interaction with these services is managed by a custom `Stub` that dynamically uses their published `/manifest` and `/schema` endpoints to structure requests and parse responses.

### Advanced Memory System
A sophisticated memory system allows the AI to be a true "creative partner."

#### Short-Term Contextual Memory
*   **Mechanism:** A rolling log of recent interactions (user inputs, LLM interpretations, summaries of generated assets) for each user session is maintained in an SQLite table (`interaction_log`).
*   **Time Window:** This log is queried for interactions within a configurable time window (e.g., the last hour) to provide immediate conversational context to the LLM.
*   **Functionality:** Enables the AI to understand references like "it" or follow-up modifications to the most recently discussed creation (e.g., "make it blue").
*   **Management:** Old entries in this log are periodically archived (deleted) to keep it performant.

#### Long-Term Persistent Memory
*   **Storage:** Detailed summaries of all successfully completed creations are stored persistently in an SQLite database (`creations` table). This includes:
    *   Original user prompt
    *   Final enhanced prompt used for generation
    *   Paths to the saved image, 3D model, and video files
    *   Extracted tags
    *   Timestamp of creation
    *   User ID
*   **Purpose:** Enables recall of any past creation.

#### Semantic Recall with FAISS
*   **Embedding:** Key textual information from each creation (e.g., a combination of the enhanced prompt and tags) is converted into a dense vector embedding using a `sentence-transformers` model (e.g., `all-MiniLM-L6-v2`).
*   **Indexing:** These embeddings are stored in a FAISS (Facebook AI Similarity Search) index, which is persisted to disk alongside a mapping to the corresponding SQLite creation IDs.
*   **Semantic Search:** When a user requests to recall a past item, their query is also embedded. FAISS is then used to find the most semantically similar past creations, even if the exact keywords don't match.
*   **Hybrid Recall:** This semantic search can be combined with keyword and date-based filtering on the SQLite data for more precise and efficient recall (e.g., "that horse-like creature I made around last Christmas").

## Technology Stack
*   **Python:** Core programming language.
*   **Openfabric pysdk:** For interacting with the Openfabric platform and services.
*   **Ollama:** For running local LLMs (e.g., DeepSeek, Llama 3, Gemma).
*   **SQLite:** For persistent structured metadata storage (long-term memory summaries and short-term interaction logs).
*   **Sentence-Transformers:** For generating text embeddings.
*   **FAISS (CPU):** For efficient semantic similarity search on embeddings.
*   **Dateparser:** For parsing human-readable date references (e.g., "last Tuesday").
*   **Requests:** For HTTP communication (used by `core.stub.Stub`).
*   **Poetry:** For Python dependency management and packaging.
*   **Docker:** For containerizing the application.
*   **Gradio :** For the interactive chat UI.

## Project Structure
```
ai-test/
├── README.md                        # This file
└── app/                             # Main application source code
    ├── config/                      # Openfabric app configuration (manifest.json, state.json, etc..)
    ├── core/                        # Core Openfabric interaction (stub.py, remote.py)
    ├── datastore/                   # Default location for SQLite DBs, FAISS index, generated assets
    │   └── production_main_assets/  # Assets from main.py runs
    ├── gradio_datastore/        # Data specific to Gradio runs
    ├── ontology_dc8f.../            # Auto-generated Python classes for app data contract
    ├── app_gradio.py                # Gradio UI application
    ├── llm_client.py                # Client for Ollama LLM
    ├── main.py                      # Openfabric app entry point (config, execute)
    ├── ignite.py                      # Openfabric server setup
    ├── memory_manager.py            # Manages short-term and long-term (SQLite+FAISS) memory
    ├── pipeline_logic.py            # Core AIProcessingPipeline orchestrator
    ├── openfabric_pysdk-0.3.0-py3-none-any.whl # Openfabric SDK wheel (if local)
    ├── Dockerfile
    ├── poetry.lock
    ├── pyproject.toml
    └── start.sh                     # Script to run the Openfabric app locally
```

## Setup and Installation

### Prerequisites
*   **Python:** Version 3.8+ (as defined in `pyproject.toml`).
*   **Poetry:** For managing Python dependencies. Installation guide: [https://python-poetry.org/docs/#installation](https://python-poetry.org/docs/#installation)
*   **Docker:** For running the containerized application. Installation guide: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)
*   **Ollama:** For running local LLMs. Download and install from [https://ollama.com/](https://ollama.com/).

### Local Setup (using Poetry)
1.  **Clone the repository:**
    ```bash
    git clone https://github.com/madhavc21/Openfabric_ImageTo3D_DualMemory.git
    cd ai-test/app
    ```
2.  **Install dependencies using Poetry:**
    > NOTE: This project involves heavy libraries such as transformers, FAISS, thus install times maybe large depending on systems, this is normal and not likely an issue with the project.
    ```bash
    poetry install --only main 
    ```
    (If you also want development dependencies like `pytest`, run `poetry install`).

### Ollama LLM Setup
1.  **Ensure Ollama is running.** Typically, you can start it by running `ollama serve` in your terminal, or it might run as a background service.
2.  **Pull a recommended LLM model.** The application is designed to be flexible, but a capable instruct-tuned or chat-tuned model is recommended for good intent parsing and JSON output. Examples:
    *   `ollama pull gemma3:1b` (Reccommended: Google's latest Gemma model, light weight, accurate and fast)
    *   `ollama pull llama3:8b-instruct` (Excellent general-purpose instruction follower)
    *   `ollama pull deepseek-coder:6.7b-instruct`
    
    The default model used can be configured (see [Configuration](#configuration) section). The `sentence-transformers` model (`all-MiniLM-L6-v2`) will be downloaded automatically by `MemoryManager` on its first run.

## Running the Application

You can run the application in two main ways:

### 1. As an Openfabric Application (Core Deliverable)

This method uses `app/main.py` as the entry point and exposes an API, typically tested via Swagger UI.

#### Using Docker (Recommended)
This is the preferred method for a consistent environment.

1.  **Build the Docker image:**
    From the project root directory (`ai-test/`):
    ```bash
    docker build -t ai-creative-partner .
    ```

2.  **Run the Docker container:**
    ```bash
    docker run --rm -p 8888:8888 \
           -e OLLAMA_BASE_URL="http://host.docker.internal:11434" \
           -e OLLAMA_MODEL_NAME="llama3:8b-instruct" \ # Or your preferred Ollama model
           -v "${PWD}/datastore:/app/datastore" \
           ai-creative-partner
    ```
    *   `-p 8888:8888`: Maps port 8888 from the container to your host.
    *   `-e OLLAMA_BASE_URL="http://host.docker.internal:11434"`: Allows the container to access Ollama running on your host machine. (For Linux hosts, you might need to use your host's IP address instead of `host.docker.internal`, e.g., `http://$(hostname -I | awk '{print $1}' -):11434`).
    *   `-e OLLAMA_MODEL_NAME="your-ollama-model-name"`: Sets the LLM model to be used by `app/main.py`.
    *   `-v "${PWD}/datastore:/app/datastore"`: Mounts the local `app/datastore` directory into the container for persistent storage of the SQLite database, FAISS index, and generated assets. Make sure the local `app/datastore` directory exists or can be created.

#### Running Locally with `start.sh`
This script typically uses `app/ignite.py` to start the Openfabric pysdk server.
1.  Ensure all Python dependencies are installed in your active environment (e.g., via `poetry install --only main`).
2.  Ensure Ollama is running and accessible at `http://localhost:11434` (or configure `OLLAMA_BASE_URL` and `OLLAMA_MODEL_NAME` as environment variables before running `start.sh`).
3.  From the project root:
    ```bash
    ./start.sh
    ```

#### Testing via Swagger UI
Once the Openfabric application is running (either via Docker or `start.sh`), you can access its API:
1.  Open your browser to `http://localhost:8888/swagger-ui/`
2.  Navigate to the `/App/execution` POST endpoint.
3.  Click "Try it out."
4.  Provide a JSON request body. For example:
    ```json
    {
      "prompt": "A futuristic city with flying vehicles at sunset",
    }
    ```
5.  Click "Execute." The response will appear in the `message` field. Check the application logs for detailed processing information.

### 2. Using the Gradio UI
An interactive Gradio chat interface is provided for a more user-friendly experience.

1.  Ensure all Python dependencies, including Gradio-specific ones (`gradio`, `Pillow`, `faster-whisper`), are installed in your active environment:
    ```bash
    poetry install
    ```
2.  Ensure Ollama and the Openfabric services (Text-to-Image, Image-to-3D) are running.
3.  Navigate to the `app/` directory and run:
    ```bash
    python app_gradio.py
    ```
4.  Open the local URL provided by Gradio (usually `http://127.0.0.1:7860`) in your browser.
5.  The Gradio app uses its own set of configurations (Ollama model, paths) defined at the top of `app_gradio.py`. It will create its own `gradio_datastore` for memory and assets, separate from `main.py`'s datastore unless paths are configured to be the same.

## Configuration

*   **Openfabric App (`main.py` context):**
    *   **Service App IDs (Hostnames):** Configured in `app/config/state.json`. The `main.py` application retrieves these via the `config()` callback from the Openfabric platform. Assumed order: Text-to-Image first, Image-to-3D second.
    *   **Ollama Base URL:** Set via the `OLLAMA_BASE_URL` environment variable (defaults to `http://localhost:11434`).
    *   **Ollama Model Name:** Set via the `OLLAMA_MODEL_NAME` environment variable (defaults to a model specified in `main.py`).
    *   **Database Path:** Hardcoded in `main.py` to `app/datastore/production_main_memory.sqlite`.
    *   **Asset Output Path:** Hardcoded in `main.py` to `app/datastore/production_main_assets/`.
*   **Gradio UI (`app_gradio.py` context):**
    *   Service App IDs, Ollama URL, Ollama Model Name, DB Path (`gradio_memory.sqlite`), FAISS Path (`gradio_creations.faiss`), and Asset Output Path (`gradio_generated_assets`) are currently hardcoded at the top of `app_gradio.py`. These create a separate data environment for the Gradio app.

## Troubleshooting
*   **`ModuleNotFoundError` in Docker:** Ensure all Python dependencies (including `numpy`, `sentence-transformers`, `faiss-cpu`, `dateparser`, `requests`, `gradio`, `faster-whisper`, `Pillow`) are correctly listed in `pyproject.toml` under `[tool.poetry.dependencies]`. Rebuild the Docker image.
*   **Ollama Connection Issues:**
    *   Ensure Ollama server is running: `ollama serve`.
    *   Verify `OLLAMA_BASE_URL` (environment variable for Docker, or hardcoded in `app_gradio.py`) is correct (e.g., `http://localhost:11434` for local, `http://host.docker.internal:11434` for Docker on Mac/Windows).
    *   Ensure the LLM model specified by `OLLAMA_MODEL_NAME` is pulled in Ollama (`ollama list`, `ollama pull <model_name>`).
*   **FAISS Errors:** Ensure `faiss-cpu` is installed correctly. The "Failed to load GPU Faiss" message is normal if using `faiss-cpu`.
*   **LLM JSON Output Issues:** If the LLM (especially smaller models) doesn't produce clean JSON for `interpret_user_request`, try a more capable instruct-tuned or chat-tuned model (e.g., `llama3:8b-instruct`, `gemma:7b-it`, larger DeepSeek models). The system prompts in `llm_client.py` may also need further tuning for your specific model.
*   **Gradio `gr.Model3D` not displaying:** Ensure the `.glb` file path returned by the pipeline is correct and accessible to the Gradio server. The `BUFFER_VIEW_TARGET_MISSING` issue in some GLBs might affect some viewers; `gr.Model3D` (BabylonJS) is often more lenient than WebKit-based viewers.

## Future Enhancements
*   More robust LLM prompt engineering for intent parsing and entity extraction.
*   Configuration of paths and model names via a central config file or more environment variables instead of hardcoding.
*   Option for user to select from multiple matches during long-term recall.
*   More sophisticated tag generation (e.g., using LLM).
*   UI for browsing and managing long-term memory directly.
*   Implementing the `LONG_TERM_RECALL_ONLY` intent more fully in the pipeline.
*   Trimesh re-export step for GLB files if compatibility issues persist with viewers.